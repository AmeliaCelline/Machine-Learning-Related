{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "u50JxmjnIkPH",
        "outputId": "73b201db-4936-4718-fae9-240eddacae79",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ZlU8khsZjZT"
      },
      "source": [
        "# **Week 4: Colab Experiment**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E3LRo3ehZo2B"
      },
      "source": [
        "# I. Introduction\n",
        "In this exercise, we load the Breast cancer wisconsin dataset for classification."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sn2Bcr9sZofG"
      },
      "source": [
        "# II. Methods\n",
        "We train 3 models:\n",
        "1. logistic regression\n",
        "2. support vector machine\n",
        "3. decision tree.\n",
        "\n",
        "..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "X4dRDQZqqiet"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "from datetime import datetime\n",
        "import numpy as np\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import zero_one_loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "ArV6oId2qjCh"
      },
      "outputs": [],
      "source": [
        "# Define the dependent and independent variables.\n",
        "data = load_breast_cancer()\n",
        "Y = data.target\n",
        "X = data.data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "_kY6lUBXL0TX"
      },
      "outputs": [],
      "source": [
        "# Create CV folds\n",
        "num_folds = 5\n",
        "kf = KFold(n_splits=num_folds, random_state=0, shuffle=True)\n",
        "kfold_indices = {}\n",
        "\n",
        "for i, (train_index, test_index) in enumerate(kf.split(X)):\n",
        "  kfold_indices[f\"fold_{i}\"] = {'train': train_index, 'test': test_index}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "UsTfhZNxL0V1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12b476b5-ed6d-4ec8-a1ec-ca5204345803"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold id  0\n",
            "logistic hyperparameter:\n",
            "{'C': 0.30000000000000004}\n",
            "svm hyperparameter:\n",
            "{'C': 0.20000000000000004, 'kernel': 'linear'}\n",
            "deicision tree hyperparameter:\n",
            "{'criterion': 'gini', 'max_depth': 5}\n",
            "\n",
            "fold id  1\n",
            "logistic hyperparameter:\n",
            "{'C': 0.20000000000000004}\n",
            "svm hyperparameter:\n",
            "{'C': 0.1, 'kernel': 'linear'}\n",
            "deicision tree hyperparameter:\n",
            "{'criterion': 'entropy', 'max_depth': 10}\n",
            "\n",
            "fold id  2\n",
            "logistic hyperparameter:\n",
            "{'C': 0.25000000000000006}\n",
            "svm hyperparameter:\n",
            "{'C': 0.1, 'kernel': 'linear'}\n",
            "deicision tree hyperparameter:\n",
            "{'criterion': 'entropy', 'max_depth': None}\n",
            "\n",
            "fold id  3\n",
            "logistic hyperparameter:\n",
            "{'C': 0.30000000000000004}\n",
            "svm hyperparameter:\n",
            "{'C': 0.5000000000000001, 'kernel': 'linear'}\n",
            "deicision tree hyperparameter:\n",
            "{'criterion': 'entropy', 'max_depth': 5}\n",
            "\n",
            "fold id  4\n",
            "logistic hyperparameter:\n",
            "{'C': 0.3500000000000001}\n",
            "svm hyperparameter:\n",
            "{'C': 0.30000000000000004, 'kernel': 'linear'}\n",
            "deicision tree hyperparameter:\n",
            "{'criterion': 'entropy', 'max_depth': 5}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Train models and apply them to the test set\n",
        "Error_rate = {'logreg': [], 'svm': [], 'decision_tree': []}\n",
        "\n",
        "#create scaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "#create models\n",
        "logistic = LogisticRegression()\n",
        "svm = SVC()\n",
        "decisionTree = DecisionTreeClassifier()\n",
        "\n",
        "#hyperparameters:\n",
        "#for logistic regression\n",
        "#by default logistic uses l2 penalty. cannot use l1 penalty since lbfgs only supports l2 penalty\n",
        "logistic_param = {\n",
        "    'C': list(np.arange(0.1, 1, 0.05)) #regularization strength where lower is stronger\n",
        "}\n",
        "\n",
        "#for SVM\n",
        "svm_param = {\n",
        "    'kernel': ['poly', 'linear', 'rbf', 'sigmoid'], #testing various kernel\n",
        "    'C': list(np.arange(0.1, 1, 0.05))#regularization strength where lower is stronger\n",
        "}\n",
        "\n",
        "#for decision tree\n",
        "decisionTree_param = {\n",
        "    'criterion': ['gini', 'entropy'], #some ways to determine which split is the best\n",
        "    'max_depth': [None, 5, 10, 20, 30, 40, 50, 100], #max depth of tree, default is none (basically no limit)\n",
        "    # 'min_samples_split': [2,3,4,5]\n",
        "\n",
        "}\n",
        "\n",
        "for fold_id in range(num_folds):\n",
        "  print(\"fold id \", fold_id)\n",
        "  X_train = X[kfold_indices[f\"fold_{fold_id}\"]['train']]\n",
        "  Y_train = Y[kfold_indices[f\"fold_{fold_id}\"]['train']]\n",
        "  X_test = X[kfold_indices[f\"fold_{fold_id}\"]['test']]\n",
        "  Y_test = Y[kfold_indices[f\"fold_{fold_id}\"]['test']]\n",
        "\n",
        "  # TODO : use standardScaler to normalize the data and run the models\n",
        "\n",
        "  # just fit using the training data. we dont do fit for the entire data\n",
        "  # because we dont want the testing data to influence the scaling process\n",
        "  scaler.fit(X_train)\n",
        "  X_train_scaled = scaler.transform(X_train)\n",
        "  X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "  # GridSearchCV(estimator, param_grid, scoring='accuracy')\n",
        "\n",
        "  #grid search to find best hyperparameters, cross validation = 5 fold\n",
        "\n",
        "  #Logistic regression\n",
        "  grid_logistic = GridSearchCV(logistic, logistic_param, scoring='accuracy', cv = 5)\n",
        "  grid_logistic.fit(X_train_scaled, Y_train)\n",
        "  updatedLogistic = grid_logistic.best_estimator_\n",
        "\n",
        "  print(\"logistic hyperparameter:\")\n",
        "  print(grid_logistic.best_params_)\n",
        "\n",
        "  Error_rate['logreg'].append(zero_one_loss(\n",
        "      Y_test, updatedLogistic.predict(X_test_scaled)))\n",
        "\n",
        "  # SVM\n",
        "  grid_svm = GridSearchCV(svm, svm_param, scoring='accuracy', cv = 5)\n",
        "  grid_svm.fit(X_train_scaled, Y_train)\n",
        "  updatedSvm = grid_svm.best_estimator_\n",
        "\n",
        "  print(\"svm hyperparameter:\")\n",
        "  print(grid_svm.best_params_)\n",
        "\n",
        "  Error_rate['svm'].append(zero_one_loss(\n",
        "      Y_test, updatedSvm.predict(X_test_scaled)))\n",
        "\n",
        "  # Decision tree\n",
        "  grid_decisionTree = GridSearchCV(decisionTree, decisionTree_param, scoring='accuracy', cv = 5)\n",
        "  grid_decisionTree.fit(X_train_scaled, Y_train)\n",
        "  updatedDecisionTree = grid_decisionTree.best_estimator_\n",
        "\n",
        "  print(\"deicision tree hyperparameter:\")\n",
        "  print(grid_decisionTree.best_params_)\n",
        "\n",
        "  print()\n",
        "  Error_rate['decision_tree'].append(zero_one_loss(\n",
        "      Y_test, updatedDecisionTree.predict(X_test_scaled)))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tW0uMLYwZ63z"
      },
      "source": [
        "## III. Results\n",
        "\n",
        "Show the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "0uD1iPyJP25T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "efd54fe0-bd0b-407e-caee-7ba28c9e725f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The error rate over 5 folds in CV:\n",
            "Logistic Regression: mean = 0.0246, std = 0.0151\n",
            "SVM: mean = 0.0281, std = 0.017\n",
            "Decision Tree: mean = 0.0686, std = 0.0205\n"
          ]
        }
      ],
      "source": [
        "# TODO\n",
        "print(f\"The error rate over 5 folds in CV:\")\n",
        "print(f\"Logistic Regression: mean = {round(np.mean(Error_rate['logreg']),4)}, std = {round(np.std(Error_rate['logreg']),4)}\")\n",
        "print(f\"SVM: mean = {round(np.mean(Error_rate['svm']),4)}, std = {round(np.std(Error_rate['svm']),4)}\")\n",
        "print(f\"Decision Tree: mean = {round(np.mean(Error_rate['decision_tree']),4)}, std = {round(np.std(Error_rate['decision_tree']),4)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srwwVH9TaBm3"
      },
      "source": [
        "# IV. Conclusion and Discussion\n",
        "\n",
        "- Logistic Regression has the lowest mean and std. This mean that logistic is able to produce the most consistent and is considered the best out of the three models. Behind it is SVM with a similar performance, then the last one is decision tree.\n",
        "- Seeing how the best kernel for SVM in this dataset is linear, and how the best model is logistic regression, it seems that the dataset has linear characteristic.\n",
        "\n",
        "\n",
        "## Extra things I discover while doing this homework\n",
        "- Adding more options or range in the grid parameters don't always mean less error.\n",
        "<br> Example: I tried adding min_samples_split hyperparameter and min_samples_leaf hyperparameters but the mean error for the decision tree increases instead of decreasing. Maybe adding more hyperparameters led the tree to overfit instead.\n",
        "- Not all the regularisation can be used by default. Example, l1 penalty does not work for default logistic regression. In order to use l1 penalty for logistic, need to change the solver to other type.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}